{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Change-Detection/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Abstract"
      ],
      "metadata": {
        "id": "pi5yzhh_RW7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task of **Ground-to-Aerial matching** caught our attention for several reasons.\n",
        "\n",
        "Primarily, we were intrigued to delve into our first work in the field of Information Retrieval and multi-view and cross-domain image analysis. The prospect of addressing a novel challenge within this domain was exciting and offered the opportunity to contribute meaningfully to the development of new methodologies and technologies.\n",
        "\n",
        "Additionally, the challenge of matching a ground image to the correct satellite image without any spatial information (such as coordinates where the picture was taken) presented a fascinating problem. The complexity of this problem lies in the significant differences in scale, angle, and appearance between ground-level and aerial images, necessitating advanced algorithms and innovative approaches to bridge the gap.\n",
        "\n",
        "Following the **feature-enrichment approach** of the [paper](https://arxiv.org/html/2404.11302v1) \"A Semantic Segmentation-guided Approach for Ground-to-Aerial Image Matching\", we present an enriched dataset, starting from a subset of the CVUSA dataset, and new models combining brand new features (such as Ground Depth estimation and Ground Semantic Segmentation), with the ones presented in the paper.\n",
        "\n",
        "As **backbone models** for our \"branches\" we used most of the state of the art models, in particular VGG16, ResNet (in the 50, 101 and 152 versions) and SAIG, a transformer-based model for dealing that proved to be [quite effective](https://arxiv.org/abs/2302.01572).\n",
        "\n",
        "The generation of the new features was done in two different Colab notebooks:\n",
        "\n",
        "\n",
        "*   [Ground Depth Estimation](https://colab.research.google.com/drive/1FzAKRESCVgZdi3U21maYJardy_BZ2Qx6?usp=drive_link)\n",
        "*   [Ground Semantic Segmentation](https://colab.research.google.com/drive/1Mu0SlsxwE-nl9W5rOCQCxvP7UQ-AITN6?usp=drive_link)\n",
        "\n",
        "Everything was done using **Pytorch Lightning**.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1BFNMRqB_wGC_cYWSbWoNL1jvsERyNGjk' />\n",
        "<figcaption>Example of matching</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "xLOgjr_hRkmL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMqOyt0ONvDV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n",
        "!pip install patool\n",
        "!pip install torchvision nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFZKHodQUoQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import csv\n",
        "import cv2\n",
        "import gdown\n",
        "import patoolib\n",
        "import inspect\n",
        "from typing_extensions import override\n",
        "from sys import version\n",
        "import abc\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler, RandomSampler, BatchSampler\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
        "\n",
        "# pytorch lighting\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.loggers.logger import Logger"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Folders Setup\n",
        "\n",
        "shutil.rmtree('/content/input', ignore_errors = True)\n",
        "os.mkdir('/content/input')\n",
        "\n",
        "shutil.rmtree('/content/output', ignore_errors = True)\n",
        "os.mkdir('/content/output')\n",
        "\n",
        "shutil.rmtree('/content/output/log', ignore_errors = True)\n",
        "os.mkdir('/content/output/log')\n",
        "\n",
        "shutil.rmtree('/content/lightning_logs', ignore_errors = True)\n",
        "os.mkdir('/content/lightning_logs')"
      ],
      "metadata": {
        "id": "XTosPQ5YcmFt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Downloading Dataset from our Google Drive\n",
        "\n",
        "# subset: 1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd\n",
        "# subset 2.0 : 11DR7zhd6wchdyt8DSkTY2JGgf_jrtf1D\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=11DR7zhd6wchdyt8DSkTY2JGgf_jrtf1D'\n",
        "output_file = '/content/input/CVUSA_subset_2_0.rar'\n",
        "output_dir = '/content/input/data'\n",
        "\n",
        "gdown.download(url, output_file)\n",
        "patoolib.extract_archive(output_file, outdir = output_dir)\n"
      ],
      "metadata": {
        "id": "gKcEFA_tcicZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f75701-1c20-49bd-d6b1-27c395dd9196",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=11DR7zhd6wchdyt8DSkTY2JGgf_jrtf1D\n",
            "From (redirected): https://drive.google.com/uc?id=11DR7zhd6wchdyt8DSkTY2JGgf_jrtf1D&confirm=t&uuid=b4d8f5e2-b3b6-4c76-996e-aef9dbef6e6e\n",
            "To: /content/input/CVUSA_subset_2_0.rar\n",
            "100%|██████████| 4.65G/4.65G [00:43<00:00, 106MB/s]\n",
            "INFO patool: Extracting /content/input/CVUSA_subset_2_0.rar ...\n",
            "INFO:patool:Extracting /content/input/CVUSA_subset_2_0.rar ...\n",
            "INFO patool: ... creating output directory `/content/input/data'.\n",
            "INFO:patool:... creating output directory `/content/input/data'.\n",
            "INFO patool: running /usr/bin/7z x -o/content/input/data -- /content/input/CVUSA_subset_2_0.rar\n",
            "INFO:patool:running /usr/bin/7z x -o/content/input/data -- /content/input/CVUSA_subset_2_0.rar\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... /content/input/CVUSA_subset_2_0.rar extracted to `/content/input/data'.\n",
            "INFO:patool:... /content/input/CVUSA_subset_2_0.rar extracted to `/content/input/data'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/input/data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings\n",
        "\n",
        "pl.seed_everything(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "uOvjHHWCdLWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426aa5bc-9ab8-4030-cb86-6a238a572b4e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "In this section we define the **CVUSADataset** class and the **CVUSADataModule** class.\n",
        "\n",
        "CVUSADataset extracts from the train_updated.csv and val_updated.csv the **paths to the images** that we will use during training.\n",
        "\n",
        "CVUSADataModule instanciates the two splits of the CVUSADataset and provides several possibilities. It deals with:\n",
        "\n",
        "1.   **Resizing** the various images\n",
        "2.   **Normalization** of the various images\n",
        "3.   Definition of the **Dataloaders**\n",
        "4.   Computation of **mean and standard deviation**\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1ekFGoQvb1jwnwu41noJddDuVzqEM7SYX' />\n",
        "<figcaption>Example of ground segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1gTYYsRrwns3taHfZ5YZQAGtSz0EQhQqF' />\n",
        "<figcaption>Example of ground depth estimation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT1pbNEXQknc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title CVUSADataset class definition\n",
        "\n",
        "# Expected dataset structure: the input_dir contains the split cvs files and a\n",
        "# subdirectory named 'data' with the CVUSA dataset\n",
        "\n",
        "class CVUSADataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_dir, split = 'train', polar = False):\n",
        "        self.split = split\n",
        "        self.polar = polar\n",
        "        self.data = self.load_data(input_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                grd_path = row[1]\n",
        "                grd_seg_path = row[5]\n",
        "                grd_depth_path = row[6]\n",
        "                if self.polar: #If we want to use polar\n",
        "                   sat_path = row[3]\n",
        "                   sat_seg_path = row[4]\n",
        "                else:\n",
        "                  sat_path = row[0]\n",
        "                  sat_seg_path = row[2]\n",
        "                data.append({\"grd_path\": grd_path, \"grd_seg_path\": grd_seg_path, \"grd_depth_path\": grd_depth_path, \"sat_path\": sat_path, \"sat_seg_path\": sat_seg_path})\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        grd_path = dictionary['grd_path']\n",
        "        grd_seg_path = dictionary['grd_seg_path']\n",
        "        grd_depth_path = dictionary['grd_depth_path']\n",
        "        sat_path = dictionary['sat_path']\n",
        "        sat_seg_path = dictionary['sat_seg_path']\n",
        "        return grd_path, grd_seg_path, grd_depth_path, sat_path, sat_seg_path\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title CVUSADataModule class definition\n",
        "\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, input_dir, polar=False, batch_size=8, grd_resize=None, grd_seg_resize=None, grd_depth_resize=None, sat_resize=None, sat_seg_resize=None, augmentations=False):\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dir = input_dir\n",
        "        self.data_dir = input_dir  # + '/data' I made this choice for semplicity!!! \"\"\"IMPORTANT!!!\"\n",
        "        self.polar = polar\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "        self.original_size = {'grd': None, 'grd_seg': None, 'grd_depth': None, 'sat': None, 'sat_seg': None}\n",
        "        self.resize = {'grd': grd_resize, 'grd_seg': grd_seg_resize, 'grd_depth': grd_depth_resize, 'sat': sat_resize, 'sat_seg': sat_seg_resize}\n",
        "        self.size = {'grd': None, 'grd_seg': None, 'grd_depth': None, 'sat': None, 'sat_seg': None}\n",
        "        self.mean = {'grd': [0, 0, 0], 'grd_seg': [0, 0, 0], 'grd_depth': [0, 0, 0], 'sat': [0, 0, 0], 'sat_seg': [0, 0, 0]}\n",
        "        self.std = {'grd': [1, 1, 1], 'grd_seg': [1, 1, 1], 'grd_depth': [1, 1, 1], 'sat': [1, 1, 1], 'sat_seg': [1, 1, 1]}\n",
        "        self.transform = {'grd': None, 'grd_seg': None, 'grd_depth': None, 'sat': None, 'sat_seg': None}\n",
        "        self.train_transform = {'grd': None, 'grd_seg': None, 'grd_depth': None, 'sat': None, 'sat_seg': None}\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        # load the datasets\n",
        "        self.train_dataset = CVUSADataset(input_dir=self.input_dir, split='train_updated', polar=self.polar)\n",
        "        self.val_dataset = CVUSADataset(input_dir=self.input_dir, split='val_updated', polar=self.polar)\n",
        "\n",
        "        # find image sizes\n",
        "        self.__compute_image_sizes()\n",
        "\n",
        "        # compute transforms\n",
        "        self.__compute_transforms()\n",
        "\n",
        "\n",
        "    def __compute_image_sizes(self):\n",
        "        grd_sample, grd_seg_sample, grd_depth_sample, sat_sample, sat_seg_sample = self.train_dataset[0]\n",
        "\n",
        "        grd_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_sample)))\n",
        "        grd_seg_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_seg_sample)))\n",
        "        grd_depth_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_depth_sample)))\n",
        "        sat_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, sat_sample)))\n",
        "        sat_seg_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, sat_seg_sample)))\n",
        "\n",
        "        self.original_size['grd'] = grd_image.size()[1:3]\n",
        "        self.original_size['grd_seg'] = grd_seg_image.size()[1:3]\n",
        "        self.original_size['grd_depth'] = grd_depth_image.size()[1:3]\n",
        "        self.original_size['sat'] = sat_image.size()[1:3]\n",
        "        self.original_size['sat_seg'] = sat_seg_image.size()[1:3]\n",
        "\n",
        "        self.size['grd'] = grd_image.size()[1:3]\n",
        "        self.size['grd_seg'] = grd_seg_image.size()[1:3]\n",
        "        self.size['grd_depth'] = grd_depth_image.size()[1:3]\n",
        "        self.size['sat'] = sat_image.size()[1:3]\n",
        "        self.size['sat_seg'] = sat_seg_image.size()[1:3]\n",
        "\n",
        "        # compute image new sizes\n",
        "        for key in self.resize:\n",
        "            if self.resize[key]:\n",
        "                image = v2.ToImage()(Image.open(os.path.join(self.data_dir, locals()[f\"{key}_sample\"])))\n",
        "                self.size[key] = v2.Resize((self.resize[key]))(image).size()[1:3]\n",
        "\n",
        "\n",
        "    def __compute_transforms(self):\n",
        "        for key in self.transform:\n",
        "            self.transform[key] = v2.Compose([\n",
        "                v2.ToImage(),\n",
        "                v2.Resize(self.size[key]),\n",
        "                v2.ToDtype(torch.float32, scale=True),\n",
        "                v2.Normalize(self.mean[key], self.std[key], inplace=False)\n",
        "            ])\n",
        "\n",
        "        self.train_transform = self.transform\n",
        "\n",
        "        if self.augmentations:\n",
        "\n",
        "            self.transform['grd'] = v2.Compose([\n",
        "                v2.ToImage(),\n",
        "                v2.Resize(self.size['grd']),\n",
        "                # add color jitter\n",
        "                # add advanced blur or sharpen\n",
        "                # add grid dropout or coarse dropout\n",
        "                v2.ToDtype(torch.float32, scale=True),\n",
        "                v2.Normalize(self.mean['grd'], self.std['grd'], inplace=False)\n",
        "            ])\n",
        "\n",
        "            self.transform['sat'] = v2.Compose([\n",
        "                v2.ToImage(),\n",
        "                v2.Resize(self.size['sat']),\n",
        "                # add color jitter\n",
        "                # add advanced blur or sharpen\n",
        "                # add grid dropout or coarse dropout\n",
        "                v2.ToDtype(torch.float32, scale=True),\n",
        "                v2.Normalize(self.mean['sat'], self.std['sat'], inplace=False)\n",
        "            ])\n",
        "\n",
        "\n",
        "    def train_collate_fn(self, batch):\n",
        "        return self.collate_fn(batch, 'train')\n",
        "\n",
        "\n",
        "    def val_collate_fn(self, batch):\n",
        "        return self.collate_fn(batch, 'val')\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch, dataset):\n",
        "        grd_path, grd_seg_path, grd_depth_path, sat_path, sat_seg_path = zip(*batch)\n",
        "\n",
        "        # load and transform each image in the batch\n",
        "        grd_ids, grd_images = self.__compute_images(grd_path, 'grd', dataset)\n",
        "        grd_seg_ids, grd_seg_images = self.__compute_images(grd_seg_path, 'grd_seg', dataset)\n",
        "        grd_depth_ids, grd_depth_images = self.__compute_images(grd_depth_path, 'grd_depth', dataset)\n",
        "        sat_ids, sat_images = self.__compute_images(sat_path, 'sat', dataset)\n",
        "        sat_seg_ids, sat_seg_images = self.__compute_images(sat_seg_path, 'sat_seg', dataset)\n",
        "\n",
        "        grd_samples = {'imgs': grd_images, 'imgs_id': grd_ids}\n",
        "        grd_seg_samples = {'imgs': grd_seg_images, 'imgs_id': grd_seg_ids}\n",
        "        grd_depth_samples = {'imgs': grd_depth_images, 'imgs_id': grd_depth_ids}\n",
        "        sat_samples = {'imgs': sat_images, 'imgs_id': sat_ids}\n",
        "        sat_seg_samples = {'imgs': sat_seg_images, 'imgs_id': sat_seg_ids}\n",
        "\n",
        "        return grd_samples, grd_seg_samples, grd_depth_samples, sat_samples, sat_seg_samples\n",
        "\n",
        "\n",
        "    def __compute_images(self, paths, img_type, dataset):\n",
        "        images = []\n",
        "        ids = []\n",
        "\n",
        "        for img_path in paths:\n",
        "            img = Image.open(os.path.join(self.data_dir, img_path))\n",
        "\n",
        "            if dataset == 'train':\n",
        "                img = self.train_transform[img_type](img)\n",
        "            else:\n",
        "                img = self.transform[img_type](img)\n",
        "\n",
        "            images.append(img)\n",
        "            ids.append(int(img_path[-11:-4]))\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "        ids_tensor = torch.tensor(ids, dtype=int)\n",
        "        return ids_tensor, images_tensor\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.train_collate_fn, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.val_collate_fn, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    def compute_mean_std(self):\n",
        "        mean_std = {}\n",
        "\n",
        "        for key in self.mean:\n",
        "            mean, std = self.__compute_mean_std_for_key(key)\n",
        "            mean_std[key + '_mean'] = mean\n",
        "            mean_std[key + '_std'] = std\n",
        "\n",
        "        return mean_std\n",
        "\n",
        "\n",
        "    def __compute_mean_std_for_key(self, key):\n",
        "        mean = np.array([0., 0., 0.])\n",
        "        std = np.array([0., 0., 0.])\n",
        "\n",
        "        for i in self.train_dataset.data:\n",
        "            img_path = os.path.join(self.data_dir, i[f'{key}_path'])\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img.astype(float) / 255.\n",
        "            img_size = img.shape[0] * img.shape[1]\n",
        "            mean += np.mean(img[:, :, :], axis=(0, 1))\n",
        "            std += ((img[:, :, :] - mean) ** 2).sum(axis=(0, 1)) / img_size\n",
        "\n",
        "        mean /= len(self.train_dataset.data)\n",
        "        std = np.sqrt(std / len(self.train_dataset.data))\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "    def set_mean_std(self, mean_std):\n",
        "        for key in self.mean:\n",
        "            self.mean[key] = mean_std[f'{key}_mean']\n",
        "            self.std[key] = mean_std[f'{key}_std']\n",
        "\n",
        "        self.__compute_transforms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkLiGNItQpOk"
      },
      "outputs": [],
      "source": [
        "# @title Creating dataloaders\n",
        "\n",
        "input_dir = '/content/input/data/CVUSA'\n",
        "polar = False\n",
        "\n",
        "data_module = CVUSADataModule(\n",
        "    input_dir = input_dir,\n",
        "    polar = polar,\n",
        "    augmentations = True,\n",
        "    batch_size = 16,\n",
        "    grd_resize = 128,\n",
        "    grd_seg_resize = 128,\n",
        "    grd_depth_resize = 128,\n",
        "    sat_resize = 96,\n",
        "    sat_seg_resize = 96\n",
        "\n",
        ")\n",
        "data_module.setup()\n",
        "#mean_std = data_module.compute_mean_std()\n",
        "# polar = False\n",
        "if not polar:\n",
        "    mean_std = {'grd_mean': [0.4691, 0.4821, 0.4603],'grd_std': [0.2202, 0.2191, 0.2583],\n",
        "                'grd_seg_mean': [0.2976, 0.7013, 0.3604],'grd_seg_std': [0.2777, 0.3306, 0.4343],\n",
        "                'grd_depth_mean': [0.3874, 0.166 , 0.1971],'grd_depth_std': [0.3763, 0.2308, 0.171 ],\n",
        "                'sat_mean': [0.3833, 0.3964, 0.3434],'sat_std': [0.1951, 0.1833, 0.1934],\n",
        "                'sat_seg_mean': [0.2861, 0.8014, 0.8299],'sat_seg_std': [0.4468, 0.3955, 0.3707]\n",
        "                }\n",
        "else:\n",
        "# polar = True\n",
        "    mean_std = {'grd_mean': [0.4691, 0.4821, 0.4603],'grd_std': [0.2202, 0.2191, 0.2583],\n",
        "                'grd_seg_mean': [0.2976, 0.7013, 0.3604],'grd_seg_std': [0.2777, 0.3306, 0.4343],\n",
        "                'grd_depth_mean': [0.3874, 0.166 , 0.1971],'grd_depth_std': [0.3763, 0.2308, 0.171 ],\n",
        "                'sat_mean': [0.4   , 0.4128, 0.3647],'sat_std': [0.1966, 0.1862, 0.1993],\n",
        "                'sat_seg_mean': [0.3349, 0.7968, 0.8518],'sat_seg_std': [0.4638, 0.3969, 0.3478]\n",
        "                }\n",
        "\n",
        "data_module.set_mean_std(mean_std)\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our models were trained using two of the most effective and used losses in similar works.\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j)/\\tau)}{\\sum_{k=1}^N \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k)/\\tau)}\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{L}_{\\text{triplet}} = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\log \\left(1 + \\exp\\left((\\text{pos_dist}_i - \\text{dist_array}_{ij}) \\cdot w\\right)\\right)\n",
        "\\end{align*}\n",
        "\n",
        "In particular, not only we treat all the other **satellite images that are incorrect as negatives** but we also conider **incorrect ground images for a given satellite as negatives**. This is done for multiple reasons:\n",
        "\n",
        "1. The model can be used also to match a certain satellite image to a ground image, making the model **multi-modal**.\n",
        "2. The model can better shape the embeddings produced in a way that each of them is ideally far from all the other **embeddings that are not the correct ones**.\n",
        "\n",
        "We combine the two losses by taking the **average**."
      ],
      "metadata": {
        "id": "R-_ElWjjXIfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation SoftMarginTripletLoss\n",
        "\n",
        "class TripletLoss(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_weight = 1.0):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "        dist_array = 2.0 - 2.0 * torch.matmul(image_features2, image_features1.T)\n",
        "        n = len(image_features1)\n",
        "        pos_dist = torch.diag(dist_array)\n",
        "        pair_n = n * (n - 1.0)\n",
        "        triplet_dist_g2s = pos_dist - dist_array\n",
        "        loss_g2s = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_g2s * self.loss_weight)))/pair_n\n",
        "        triplet_dist_s2g = torch.unsqueeze(pos_dist, 1) - dist_array\n",
        "        loss_s2g = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_s2g * self.loss_weight)))/pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "kuwQsXRGMI0L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation  InfoNCE Loss\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function, logit_scale=3.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function #we can use a generic loss function!\n",
        "        self.logit_scale = logit_scale\n",
        "\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        # use pairwise_cosine_similarity instead? it's the same?\n",
        "        logits_per_image1 = self.logit_scale * image_features1 @ image_features2.T\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ukTOSR3IgPf7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u6wnToFjtlF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1):\n",
        "\n",
        "    num_samples = len(emb1)\n",
        "\n",
        "    if k > num_samples :\n",
        "      return 0.0 # might happen at the end of the dataset (batch less then the chosen one)\n",
        "\n",
        "    # replace with pairwise_cosine?\n",
        "    emb1 = F.normalize(emb1, dim=-1)\n",
        "    emb2 = F.normalize(emb2, dim=-1)\n",
        "    dist_matrix = 1 - (emb1 @ emb2.T)\n",
        "\n",
        "    _, topk_indices = torch.topk(dist_matrix, k = k, dim = 1, largest = False)\n",
        "\n",
        "    correct_in_topk = sum([i in topk_indices[i, :] for i in range(num_samples)])\n",
        "\n",
        "    accuracy = correct_in_topk / num_samples\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation of Attention operator\n",
        "class Attention(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MvHm7d1ICYme",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Implementation ModelCheckpoint\n",
        "class MyModelCheckpoint(ModelCheckpoint):\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.config = config\n",
        "\n",
        "    def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
        "        if not self.dirpath:\n",
        "            raise ValueError(\"dirpath must be specified in ModelCheckpoint\")\n",
        "        file_path = os.path.join(os.path.dirname(self.dirpath), \"config.txt\")\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(self.config)\n",
        "            f.write('\\n')\n",
        "\n",
        "        return super().on_train_start(trainer, pl_module)"
      ],
      "metadata": {
        "id": "viL8vSAwncJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Branches"
      ],
      "metadata": {
        "id": "rXEBXiWchDAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we defined the backbone models that will be employed in the creation of our models.\n",
        "\n",
        "We implemented:\n",
        "\n",
        "1. **VGG16**: a well-known Convolutional Neural Network (CNN) that was introduced by the Visual Geometry Group (VGG) at the University of Oxford. It is known for its simplicity and depth, consisting of 16 layers with a fixed architecture using small convolution filters (3x3) throughout the network. VGG16 is widely used for image classification tasks and serves as a strong baseline model due to its straightforward design and effectiveness.\n",
        "\n",
        "2. **ResNet**: ResNet addresses the vanishing gradient problem by using residual blocks, which allow the model to learn residual functions with reference to the layer inputs, rather than learning unreferenced functions.\n",
        "We worked with 50, 101 and 152 versions.\n",
        "\n",
        "3. **SAIG**: SAIG model integrates self-attention mechanisms into CNNs to capture long-range dependencies and contextual information more effectively than traditional convolutional operations alone. This approach combines the strengths of CNNs in extracting local features with the ability of self-attention to model global relationships within the data, making it suitable for tasks requiring a high level of detail and contextual understanding."
      ],
      "metadata": {
        "id": "DF2JabV5ia4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VGG16\n",
        "\n",
        "class VGG_net(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, architecture, num_max_pooling, in_channels=3, output_dim=1000):\n",
        "        super(VGG_net, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.architecture = architecture\n",
        "        self.num_max_pooling = num_max_pooling\n",
        "        self.in_channels = in_channels\n",
        "        self.output_dim = output_dim\n",
        "        self.division = 2 ** num_max_pooling\n",
        "\n",
        "        self.conv_layers = self.create_conv_layers(architecture)\n",
        "\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(128 * img_size[0]//self.division * img_size[1]//self.division, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, output_dim),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps=False):\n",
        "        for layer in self.conv_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "            if featuremaps and x.shape[2]*x.shape[1] == 48*48:\n",
        "                return x\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(2, 2)]\n",
        "\n",
        "        return nn.ModuleList(layers)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"VGG_net(img_size={self.img_size}, architecture={self.architecture}, num_max_pooling={self.num_max_pooling}, in_channels={self.in_channels}, output_dim={self.output_dim})\""
      ],
      "metadata": {
        "id": "mTFXxK0cb51w",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abAMZR3-HJv4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Resnet\n",
        "\n",
        "class ResNetBranch(pl.LightningModule):\n",
        "    def __init__(self, output_dim, resnet_version=50):\n",
        "        super(ResNetBranch, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.resnet_version = resnet_version\n",
        "\n",
        "        if resnet_version == 50:\n",
        "            self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        elif resnet_version == 101:\n",
        "            self.resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "        elif resnet_version == 152:\n",
        "            self.resnet = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported ResNet version. Choose from 50, 101, or 152.\")\n",
        "\n",
        "        # Modify the last layer for the specific task\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, self.output_dim)\n",
        "\n",
        "    def forward(self, x, featuremaps=False):\n",
        "        # To print the featuremap we need to return the last conv layer output\n",
        "        if featuremaps:\n",
        "            for name, layer in list(self.resnet.named_children())[:-2]:\n",
        "                x = layer(x)\n",
        "                if x.shape[2]*x.shape[1]<48*48: #magic number: dimension 16 for visualization\n",
        "                  break\n",
        "            return x\n",
        "        else:\n",
        "            return self.resnet(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"ResNetBranch(output_dim={self.output_dim}, resnet_version={self.resnet_version})\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SAIG\n",
        "\n",
        "\n",
        "class ConvBnReluBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBnReluBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "###\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop=0.,\n",
        "        attn_drop=0.,\n",
        "        dropout=0.,\n",
        "        norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop\n",
        "        )\n",
        "        # check what is droppath\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        return x\n",
        "\n",
        "###\n",
        "\n",
        "class SAIGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, patch_size=16, in_channels=3, embed_dim=768, num_heads = 8, depth = 4, smd_dim = 8, qkv_bias = True, qk_scale = None, drop_rate=0., attn_drop_rate=0., norm_layer=None, flatten=True):\n",
        "        super(SAIGBranch, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = depth\n",
        "        self.smd_dim = smd_dim\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.qk_scale = qk_scale\n",
        "        self.drop_rate = drop_rate\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.norm_layer = norm_layer\n",
        "        self.flatten = flatten\n",
        "\n",
        "        #potremmo salvare i parametri, ha qualche senso?\n",
        "        self.output_dim = embed_dim * smd_dim\n",
        "\n",
        "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
        "\n",
        "        if img_size[0] % patch_size != 0 or img_size[1] % patch_size != 0:\n",
        "          print(\"Warning: image size is not divisible for patch size\")\n",
        "\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "        self.conv_bn_relu_blocks = nn.Sequential(\n",
        "            ConvBnReluBlock(in_channels = self.in_channels, out_channels = 64, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 64, out_channels = 128, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 128, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 256, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 256, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 512, stride = 2),\n",
        "        )\n",
        "        self.patch_block = nn.Conv2d(in_channels = 512, out_channels = embed_dim, kernel_size=1, stride=1 ,padding=0)\n",
        "        self.attn_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        #self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.num_patches))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        #self.GAP = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        #self.logits = nn.Linear(in_features = embed_dim, out_features = 512)\n",
        "\n",
        "        self.smd = nn.Sequential(\n",
        "            nn.Linear(self.num_patches, self.num_patches*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.num_patches*4, self.num_patches),\n",
        "            nn.Linear(self.num_patches, smd_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "      # extract patch embeddings\n",
        "      x = self.conv_bn_relu_blocks(x)\n",
        "\n",
        "      if featuremaps:\n",
        "        return x\n",
        "\n",
        "      x = self.patch_block(x)\n",
        "      x = x.flatten(2).transpose(1,2)\n",
        "      #x = self.norm(x) CHECK\n",
        "\n",
        "      # add position embeddings\n",
        "      x = x + self.pos_embed\n",
        "      x = self.pos_drop(x)\n",
        "\n",
        "      # pass through sequence of attention blocks\n",
        "      for blk in self.attn_blocks:\n",
        "          x = blk(x)\n",
        "\n",
        "      x = self.norm(x)\n",
        "      # x = self.GAP(x.transpose(-1, -2)).squeeze(2)\n",
        "      # x = self.logits(x)\n",
        "\n",
        "      # if featuremaps:\n",
        "      #   return x.resize(x.shape[0], self.grid_size[0], self.grid_size[1], 384)\n",
        "\n",
        "      # x: b x 88 x 384\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = self.smd(x)\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = x.flatten(-2, -1)\n",
        "\n",
        "      return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SAIGBranch(img_size={self.img_size}, patch_size={self.patch_size}, in_channels={self.in_channels}, embed_dim={self.embed_dim}, num_heads={self.num_heads}, depth={self.depth}, smd_dim={self.smd_dim}, qkv_bias={self.qkv_bias}, qk_scale={self.qk_scale}, drop_rate={self.drop_rate}, attn_drop_rate={self.attn_drop_rate}, norm_layer={self.norm_layer}, flatten={self.flatten})\""
      ],
      "metadata": {
        "id": "QDzQtqQOHzU-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generic Model"
      ],
      "metadata": {
        "id": "wNh1Gez5-j8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implemented the **generic model** class, which will be inherited by every model we will employ later.\n",
        "\n",
        "Some methods are left abstract and should be implemented following certain rules:\n",
        "\n",
        "* the **forward** function takes the whole batch as input, it has to divide it,\n",
        "apply the branches and return 3 elements: a tuple with all the output of all\n",
        "the embeddings, the total ground embedding and the total satellite embedding\n",
        "\n",
        "* the **compute_loss** function takes the tuple with all the embeddings and the\n",
        "two total embeddings as input, compute the loss and return it.\n",
        "\n",
        "* the __repr__ function returns a string representing the model"
      ],
      "metadata": {
        "id": "GuDuS7IUhBpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Multi-Branch Model\n",
        "\n",
        "class MultiBranchModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss):\n",
        "        super(MultiBranchModel, self).__init__()\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "        self.grd_features_train = []\n",
        "        self.sat_features_train = []\n",
        "\n",
        "        self.grd_features_val =  []\n",
        "        self.sat_features_val =  []\n",
        "\n",
        "        self.grd_features_test = []\n",
        "        self.sat_features_test = []\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, batch):\n",
        "        return\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        return\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        embeddings, grd_emb, sat_emb = self(batch)\n",
        "\n",
        "        loss = self.compute_loss(embeddings, grd_emb, sat_emb)\n",
        "        top_1 = top_k_rank_accuracy(grd_emb, sat_emb, k=1)\n",
        "\n",
        "        self.log('train_top1', top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, grd_emb, sat_emb = self(batch)\n",
        "\n",
        "        self.grd_features_val.append(grd_emb)\n",
        "        self.sat_features_val.append(sat_emb)\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "\n",
        "      grd_features_val = torch.cat(self.grd_features_val, dim=0)\n",
        "      sat_features_val = torch.cat(self.sat_features_val, dim=0)\n",
        "\n",
        "      num_samples = grd_features_val.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=percent1)\n",
        "\n",
        "      self.log('val_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      self.grd_features_val.clear()\n",
        "      self.sat_features_val.clear()\n",
        "      del grd_features_val, sat_features_val\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, grd_emb, sat_emb = self(batch)\n",
        "\n",
        "        self.grd_features_test.append(grd_emb)\n",
        "        self.sat_features_test.append(sat_emb)\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "\n",
        "      grd_features_test = torch.cat(self.grd_features_test, dim = 0)\n",
        "      sat_features_test = torch.cat(self.sat_features_test, dim = 0)\n",
        "\n",
        "      num_samples = grd_features_test.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=percent1)\n",
        "\n",
        "      self.log('test_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      self.grd_features_test.clear()\n",
        "      self.sat_features_test.clear()\n",
        "      del grd_features_test, sat_features_test\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def __repr__(self):\n",
        "        return"
      ],
      "metadata": {
        "id": "ap-bDZT2-mpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Dual Model (RGB Grd | RGB Sat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1GiA7Bi3xAkRg5j_NiimIqaVc4v5XujXZ' />\n",
        "<figcaption>Dual Model architecture</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "The architecture is proposed as the baseline model. It uses two branches to extract features of the ground and of the satellite images, and projects them into a latent space, describing them as vectors.\n",
        "\n",
        "To match the correct images, we compute the distances between the descriptors of the satellite images and the vectors of the ground images. Specifically, we create a distance matrix where each element represents the distance between a descriptor from a satellite image and a vector from a ground image. By calculating these distances, we can identify pairs of satellite and ground images that are closest to each other in the latent space, thereby matching the images accurately."
      ],
      "metadata": {
        "id": "l2--P6cntwrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dual Model\n",
        "\n",
        "class DualModel(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "    ):\n",
        "        super(DualModel, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch1.output_dim != self.branch2.output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch1.output_dim\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_imgs, _, _, sat_imgs, _ = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_emb = self.branch1(grd_imgs['imgs'])\n",
        "        sat_emb = self.branch2(sat_imgs['imgs'])\n",
        "\n",
        "        return (grd_emb, sat_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DualModel(model_grd={self.branch1}, model_sat={self.branch2}, loss={self.loss})\""
      ],
      "metadata": {
        "id": "IYilREkEAmgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Model\n",
        "\n",
        "branch_type = 'resnet'\n",
        "\n",
        "###\n",
        "\n",
        "if branch_type == 'resnet':\n",
        "\n",
        "    grd_model = ResNetBranch(output_dim=1024, resnet_version = 50)\n",
        "    sat_model = ResNetBranch(output_dim=1024,  resnet_version = 50)\n",
        "\n",
        "\n",
        "elif branch_type == 'saig':\n",
        "\n",
        "    grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8\n",
        "    )\n",
        "\n",
        "    sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8\n",
        "    )\n",
        "\n",
        "\n",
        "elif branch_type == 'vgg':\n",
        "\n",
        "    # Output channel of each layer in the convolution layers\n",
        "    # \"M\" stands for maxpooling layer\n",
        "    #VGG16 = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
        "    #VGG16 = [32, 32, \"M\", 64, 64, \"M\", 128, 128, 128, \"M\", 256, 256, 256, \"M\", 256, 256, 256, \"M\"]\n",
        "    VGG16 = [16, 16, \"M\", 32, 32, \"M\", 64, 64, 64, \"M\", 128, 128, 128, \"M\", 128, 128, 128, \"M\"]\n",
        "\n",
        "    grd_model = VGG_net(data_module.size['grd'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "    sat_model = VGG_net(data_module.size['sat'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "else:\n",
        "\n",
        "  raise ValueError(\"Should specify some model between the chosable ones\")\n",
        "\n",
        "###\n",
        "\n",
        "model = DualModel(grd_model, sat_model)"
      ],
      "metadata": {
        "id": "fs24fr2ZwAxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c95a726-62b0-41a7-fd9d-1ca4c4a8ca6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 123MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVD0l4tD4Byc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fef32eb-9a0e-4f96-e60c-3522060da08a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "# @title Create Trainer\n",
        "\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"dual_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='dual_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=3, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3,\n",
        "    default_root_dir = \"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p57rNWjHN1hh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "37d42b4eebd2458b90d8482751880a19",
            "075e43ab72194bbdae38d001dc2a6f2e"
          ]
        },
        "outputId": "abece5ee-1ab4-4df5-de3f-b491913070c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: tb_logs/dual_model\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ loss    │ InfoNCE      │      0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch1 │ ResNetBranch │ 25.6 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ branch2 │ ResNetBranch │ 25.6 M │\n",
              "└───┴─────────┴──────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ loss    │ InfoNCE      │      0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch1 │ ResNetBranch │ 25.6 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ branch2 │ ResNetBranch │ 25.6 M │\n",
              "└───┴─────────┴──────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 51.2 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 51.2 M                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 204                                                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 51.2 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 51.2 M                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 204                                                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37d42b4eebd2458b90d8482751880a19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Train\n",
        "\n",
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dubja7aKNutk",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "00d6f985ccf14efa953b9e67322637dc",
            "c7e1e1b557464f1db99d930b9d04d0e0"
          ]
        },
        "outputId": "a0b7c192-77ab-49fd-d2f9-8fdaf11793a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00d6f985ccf14efa953b9e67322637dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top1        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00045146726188249886  \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top1%        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.030699774622917175   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top10        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.012189616449177265   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top3        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0031602708622813225  \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top1         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00045146726188249886   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top1%         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.030699774622917175    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top10         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.012189616449177265    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top3         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0031602708622813225   </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_top1': 0.00045146726188249886,\n",
              "  'test_top3': 0.0031602708622813225,\n",
              "  'test_top10': 0.012189616449177265,\n",
              "  'test_top1%': 0.030699774622917175}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# @title Test\n",
        "\n",
        "trainer.test(\n",
        "    model = model,\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQpKOJZarI6"
      },
      "source": [
        "# Dual Model (ALL Grd | ALL Sat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dual Model with all features\n",
        "\n",
        "class DualModel_all(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "    ):\n",
        "        super(DualModel_all, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch1.output_dim != self.branch2.output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch1.output_dim\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_img, grd_seg, grd_depth, sat_img, sat_seg = batch\n",
        "        grd_tot = torch.cat((grd_img['imgs'], grd_seg['imgs'], grd_depth['imgs']), dim = 1)\n",
        "        sat_tot = torch.cat((sat_img['imgs'], sat_seg['imgs']), dim = 1)\n",
        "\n",
        "        # apply the models\n",
        "        grd_emb = self.branch1(grd_tot)\n",
        "        sat_emb = self.branch2(sat_tot)\n",
        "\n",
        "        return (grd_emb, sat_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DualModel(model_grd={self.branch1}, model_sat={self.branch2}, loss={self.loss})\""
      ],
      "metadata": {
        "id": "iDykf1JZdeIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Model\n",
        "\n",
        "branch_type = 'saig'\n",
        "\n",
        "###\n",
        "\n",
        "if branch_type == 'resnet':\n",
        "\n",
        "    grd_model = ResNetBranch(output_dim=128, resnet_version = 50)\n",
        "    sat_model = ResNetBranch(output_dim = 128,  resnet_version = 50)\n",
        "\n",
        "\n",
        "elif branch_type == 'saig':\n",
        "\n",
        "    grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8,\n",
        "        in_channels = 9\n",
        "    )\n",
        "\n",
        "    sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8,\n",
        "        in_channels = 6\n",
        "    )\n",
        "\n",
        "\n",
        "elif branch_type == 'vgg':\n",
        "\n",
        "    # Output channel of each layer in the convolution layers\n",
        "    # \"M\" stands for maxpooling layer\n",
        "    #VGG16 = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
        "    #VGG16 = [32, 32, \"M\", 64, 64, \"M\", 128, 128, 128, \"M\", 256, 256, 256, \"M\", 256, 256, 256, \"M\"]\n",
        "    VGG16 = [16, 16, \"M\", 32, 32, \"M\", 64, 64, 64, \"M\", 128, 128, 128, \"M\", 128, 128, 128, \"M\"]\n",
        "\n",
        "    grd_model = VGG_net(data_module.size['grd'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "    sat_model = VGG_net(data_module.size['sat'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "  raise ValueError(\"Should specify some model between the chosable ones\")\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "model = DualModel_all(grd_model, sat_model)"
      ],
      "metadata": {
        "id": "8XHHxUnbarI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e1054a-7acf-4ec2-9958-4240b8f9173c",
        "id": "TIaRk-C6arI6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "# @title Create Trainer\n",
        "\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"dual_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='dual_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=3, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3,\n",
        "    default_root_dir = \"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3fb94062-68b6-4c02-b444-538bd3fb19f5",
        "id": "BMjC0JgUarI7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[37mEpoch 0/29\u001b[0m \u001b[38;2;98;6;224m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m37/416\u001b[0m \u001b[38;5;245m0:00:29 • 0:04:23\u001b[0m \u001b[38;5;249m1.45it/s\u001b[0m \u001b[37mv_num: 3.000 train_top1_step: 0.125\u001b[0m\n",
              "                                                                                \u001b[37mtrain_loss_step: 2.377             \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/29</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">37/416</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:29 • 0:04:23</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.45it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 3.000 train_top1_step: 0.125</span>\n",
              "                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: 2.377             </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Train\n",
        "\n",
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303,
          "referenced_widgets": [
            "d0debe9663844313aecbdfad4f3beb05",
            "8941ac94632446edacfb19b79293caec"
          ]
        },
        "outputId": "4f7ef7d4-a6b0-40bc-b25d-5356a897c684",
        "id": "267LCO1WarI7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0debe9663844313aecbdfad4f3beb05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top1        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00045146726188249886  \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top1%        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.014446952380239964   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top10        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.008126410655677319   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top3        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.003611738095059991   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top1         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00045146726188249886   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top1%         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.014446952380239964    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top10         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.008126410655677319    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top3         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.003611738095059991    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_top1': 0.00045146726188249886,\n",
              "  'test_top3': 0.003611738095059991,\n",
              "  'test_top10': 0.008126410655677319,\n",
              "  'test_top1%': 0.014446952380239964}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# @title Test\n",
        "\n",
        "trainer.test(\n",
        "    model = model,\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triple Model (RGB Grd | RGB + SEG Sat) and (RGB Sat | RGB + SEG Grd)"
      ],
      "metadata": {
        "id": "ZGDT8x2WPpbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=17ql5CPDt4zYVDweuADsO5brW-ikqGzXK' />\n",
        "<figcaption>Triple Model architecture using satellite segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "The architecture is proposed as a slight modification of the architecture used in the original paper. It uses three branches to extract features of the ground, satellite images and satellite segmentation images, and projects them into a latent space, describing them as vectors.\n",
        "\n",
        "Differently from the Dual Model, me first concatenate (using also a fully connected layer to make differentiable such operation) the embeddings of the two satellite images, then as for the dual model we evaluate the distance matrix over such vectors.\n",
        "\n",
        "Another difference from the previous model is the possibility of defining an auxilary loss that works on the embeddings produced by the two sub-branches related to satellite images. By minimizing such a loss the idea is that the model is able to learn a good representation of such embeddings in a more direct way making them less dependent on layers of the network that appear later. During our tentatives, the models using such auxilary losses exhibited better performances in early epoches."
      ],
      "metadata": {
        "id": "-RqMbboA0RLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1bEYjAd5i-B5YQ1GHsbDkZ12a0dYx28--' />\n",
        "<figcaption>Triple Model architecture using ground segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "An alternative approach with the triple model was tried. The proposed alternative architecture uses three branches to extract features of the ground, ground segmentation images, satellite images, and projects them into a latent space, describing them as vectors.\n",
        "\n",
        "Like the previous model, we concatenate the embeddings to then compare the vectors describing the input images to match the right images.\n",
        "\n",
        "Even in this case, we tried to employ an auxilary loss (in this case defined on the embeddings resulting from the ground sub-branches)."
      ],
      "metadata": {
        "id": "2ARdskEYANcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Triple Model satellite using multiple losses\n",
        "\n",
        "class TripleModel_sat(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        model_sat_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        fully_concat=True,\n",
        "        multiple_losses=True,\n",
        "        loss_decay_rate = 0.65\n",
        "    ):\n",
        "        super(TripleModel_sat, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "        self.branch3 = model_sat_seg\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch1.output_dim != (self.branch2.output_dim + self.branch3.output_dim):\n",
        "            raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch1.output_dim\n",
        "\n",
        "        self.fully_concat = fully_concat\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # To make concatenation learnable\n",
        "        if fully_concat:\n",
        "            self.fc = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.loss_decay_rate = loss_decay_rate #used for multiple losses only\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_imgs, _, _, sat_imgs, sat_seg = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_emb = self.branch1(grd_imgs['imgs'])\n",
        "        sat_rgb_emb = self.branch2(sat_imgs['imgs'])\n",
        "        sat_seg_emb = self.branch3(sat_seg['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        sat_emb = torch.cat((sat_rgb_emb, sat_seg_emb), dim=1)\n",
        "        if self.fully_concat:\n",
        "            sat_emb = self.fc(sat_emb)\n",
        "\n",
        "        return (grd_emb, sat_rgb_emb, sat_seg_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[1], embeddings[2])\n",
        "            #update the decay\n",
        "            self.loss_decay_rate*=self.loss_decay_rate\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"TripleModel_sat(model_grd={self.branch1}, model_sat={self.branch2},\n",
        "        model_sat_seg={self.branch3}, loss={self.loss}, fully_concat={self.fully_concat},\n",
        "        multiple_losses={self.multiple_losses}, loss_decay_rate = {self.loss_decay_rate})\"\"\""
      ],
      "metadata": {
        "id": "GJBtgoPJUyyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Triple Model ground with multiple losses\n",
        "class TripleModel_grd(MultiBranchModel):\n",
        "\n",
        "    def __init__(self, model_grd, model_sat, model_grd_seg, loss=InfoNCE(loss_function=nn.CrossEntropyLoss()), fully_concat=True, multiple_losses=True, loss_decay_rate = 0.65):\n",
        "        super(TripleModel_grd, self).__init__()\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_grd_seg\n",
        "        self.branch3 = model_sat\n",
        "\n",
        "        # Verify that grd output dim is coherent with the sum of the other two models\n",
        "        if self.branch3.output_dim != (self.branch2.output_dim + self.branch1.output_dim):\n",
        "            raise ValueError(\"ATTENTION, MISMATCHING OUTPUT DIMENSIONS FOR THE BRANCHES! Must have output_dim1 = output_dim2 + output_dim3\")\n",
        "\n",
        "        self.output_dim = self.branch3.output_dim\n",
        "        self.fully_concat = fully_concat\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        if fully_concat:  # to make concatenation learnable\n",
        "            self.fc = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.loss_decay_rate = loss_decay_rate #used for multiple losses only\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_imgs, grd_seg, _, sat_imgs, _ = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_rgb_emb = self.branch1(grd_imgs['imgs'])\n",
        "        grd_seg_emb = self.branch2(grd_seg['imgs'])\n",
        "        sat_emb = self.branch3(sat_imgs['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        grd_emb = torch.cat((grd_rgb_emb, grd_seg_emb), dim=1)\n",
        "        if self.fully_concat:\n",
        "            grd_emb = self.fc(grd_emb)\n",
        "\n",
        "        return (grd_rgb_emb, grd_seg_emb, sat_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[0], embeddings[1])\n",
        "\n",
        "            #update the decay\n",
        "            self.loss_decay_rate*=self.loss_decay_rate\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"TripleModel_grd(model_grd={self.branch1}, model_sat={self.branch2},\n",
        "        model_sat_seg={self.branch3}, loss={self.loss}, fully_concat={self.fully_concat},\n",
        "        multiple_losses={self.multiple_losses}, loss_decay_rate = {self.loss_decay_rate})\"\"\""
      ],
      "metadata": {
        "id": "NVkdN6KHiMaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Model\n",
        "grd_model = ResNetBranch(output_dim = 512, resnet_version = 50) #LOOKS PROMISING!!!\n",
        "sat_model = ResNetBranch(output_dim = 256,  resnet_version = 50)\n",
        "seg_model = ResNetBranch(output_dim = 256,  resnet_version = 50)\n",
        "\n",
        "\"\"\"grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 384,\n",
        "        num_heads = 8,\n",
        "        depth = 4,\n",
        "        smd_dim = 12\n",
        "    )\n",
        "\n",
        "sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 384,\n",
        "        num_heads = 8,\n",
        "        depth = 4,\n",
        "        smd_dim = 6\n",
        "    )\n",
        "\n",
        "seg_model = SAIGBranch(\n",
        "        data_module.size['sat_seg'],\n",
        "        embed_dim= 384,\n",
        "        num_heads = 8,\n",
        "        depth = 4,\n",
        "        smd_dim = 6\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "model = TripleModel_sat(model_grd=grd_model, model_sat=sat_model, model_sat_seg=seg_model, loss = InfoNCE(loss_function = nn.CrossEntropyLoss(), logit_scale = 5.0))"
      ],
      "metadata": {
        "id": "gG7fYg6cSrji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Trainer\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"triple_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='triple_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs = 100,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3\n",
        ")"
      ],
      "metadata": {
        "id": "iHcC07AUUhtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b845e6c6-458a-4ab8-c6b3-4049a888fce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ],
      "metadata": {
        "id": "OWIVZLEGUlJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "287eaf59c6ca428ba5febacfab676fa0",
            "86b79eba4b78405c84a186fdca6ad3b3"
          ]
        },
        "outputId": "aec79545-0015-4a4c-c532-296bf699096f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ loss    │ InfoNCE      │      0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch1 │ ResNetBranch │ 24.6 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ branch2 │ ResNetBranch │ 24.0 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ branch3 │ ResNetBranch │ 24.0 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ fc      │ Linear       │  262 K │\n",
              "└───┴─────────┴──────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ loss    │ InfoNCE      │      0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch1 │ ResNetBranch │ 24.6 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ branch2 │ ResNetBranch │ 24.0 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ branch3 │ ResNetBranch │ 24.0 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ fc      │ Linear       │  262 K │\n",
              "└───┴─────────┴──────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 72.9 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 72.9 M                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 291                                                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 72.9 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 72.9 M                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 291                                                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "287eaf59c6ca428ba5febacfab676fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a \n",
              "cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: \n",
              "CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
              "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a \n",
              "cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: \n",
              "CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
              "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test\n",
        "trainer.test(\n",
        "    dataloaders = val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "7-d3ympNUoWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303,
          "referenced_widgets": [
            "3ec9b350c9af40f6b10ed9a476873c82",
            "5172e04dbc31440aa3cf65040c50830f"
          ]
        },
        "outputId": "a3b6a200-c855-4a3a-b5fd-5bc34a08857e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at tb_logs/triple_model/version_0/checkpoints/triple_model-epoch=0-val_top1=0.02.ckpt\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from the checkpoint at tb_logs/triple_model/version_0/checkpoints/triple_model-epoch=0-val_top1=0.02.ckpt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ec9b350c9af40f6b10ed9a476873c82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top1        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.016252821311354637   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top1%        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2762979567050934    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top10        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14356659352779388   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top3        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.05282166972756386   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top1         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.016252821311354637    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top1%         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2762979567050934     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top10         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14356659352779388    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top3         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.05282166972756386    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_top1': 0.016252821311354637,\n",
              "  'test_top3': 0.05282166972756386,\n",
              "  'test_top10': 0.14356659352779388,\n",
              "  'test_top1%': 0.2762979567050934}]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quadruple Model (RGB + SEG Grd | RGB + SEG Sat)"
      ],
      "metadata": {
        "id": "h9YaK9QS2VN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1kza9BhLzN6aHlbb-YhIUzlCD07Vv0hBW' />\n",
        "<figcaption>Quadruple Model architecture using ground and satellite segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This model further extends the previous approaches, combining all the possible features we have about segmentation of both Ground and Satellite images.\n",
        "\n",
        "As for the previous models, the use of auxilary Losses improved the learning of the models.\n",
        "\n"
      ],
      "metadata": {
        "id": "NrjyQvVzB791"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Quadruple Model\n",
        "\n",
        "class QuadrupleModel(MultiBranchModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_grd_seg,\n",
        "        model_sat,\n",
        "        model_sat_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        multiple_losses=True,\n",
        "        fully_concat_grd=True,\n",
        "        fully_concat_sat=True,\n",
        "        loss_decay_rate = 0.65\n",
        "    ):\n",
        "        super(QuadrupleModel, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_grd_seg\n",
        "        self.branch3 = model_sat\n",
        "        self.branch4 = model_sat_seg\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch1.output_dim + self.branch2.output_dim != self.branch3.output_dim + self.branch4.output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch1.output_dim + self.branch2.output_dim\n",
        "\n",
        "        self.fully_concat_grd = fully_concat_grd\n",
        "        self.fully_concat_sat = fully_concat_sat\n",
        "\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # To make concatenations learnable\n",
        "        if self.fully_concat_grd:\n",
        "          self.fc_grd = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        if self.fully_concat_sat:\n",
        "          self.fc_sat = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.loss_decay_rate = loss_decay_rate\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_img, grd_seg, _, sat_img, sat_seg = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_rgb_emb = self.branch1(grd_img['imgs'])\n",
        "        grd_seg_emb = self.branch2(grd_seg['imgs'])\n",
        "        sat_rgb_emb = self.branch3(sat_img['imgs'])\n",
        "        sat_seg_emb = self.branch4(sat_seg['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        grd_emb = torch.cat((grd_rgb_emb, grd_seg_emb), dim=1)\n",
        "        sat_emb = torch.cat((sat_rgb_emb, sat_seg_emb), dim=1)\n",
        "        if self.fully_concat_grd:\n",
        "          grd_emb = self.fc_grd(grd_emb)\n",
        "        if self.fully_concat_sat:\n",
        "          sat_emb = self.fc_sat(sat_emb)\n",
        "\n",
        "        return (grd_rgb_emb, grd_seg_emb, sat_rgb_emb, sat_seg_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[0], embeddings[1])\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[2], embeddings[3])\n",
        "\n",
        "            #update the decay\n",
        "            self.loss_decay_rate*=self.loss_decay_rate\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"QuadrupleModel(model_grd={self.branch1}, model_grd_seg={self.branch2},\n",
        "        model_sat={self.branch3}, model_sat_seg={self.branch4}, loss={self.loss},\n",
        "        multiple_losses={self.multiple_losses}, fully_concat_grd={self.fully_concat_grd},\n",
        "        fully_concat_sat={self.fully_concat_sat}, loss_decay_rate = {self.loss_decay_rate})\"\"\""
      ],
      "metadata": {
        "id": "q0pz4W3jYN95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Model\n",
        "\n",
        "grd_model = ResNetBranch(output_dim = 512, resnet_version = 101) # LOOKS PROMISING!!!\n",
        "grd_seg_model = ResNetBranch(output_dim = 512, resnet_version = 101)\n",
        "sat_model = ResNetBranch(output_dim = 512,  resnet_version = 101)\n",
        "sat_seg_model = ResNetBranch(output_dim = 512,  resnet_version = 101)\n",
        "\n",
        "\"\"\"grd_model = SAIGBranch(\n",
        "    data_module.size['grd'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "grd_seg_model = SAIGBranch(\n",
        "    data_module.size['grd_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "sat_model = SAIGBranch(\n",
        "    data_module.size['sat'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "sat_seg_model = SAIGBranch(\n",
        "    data_module.size['sat_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\"\"\"\n",
        "\n",
        "model = QuadrupleModel(model_grd=grd_model, model_grd_seg=grd_seg_model, model_sat=sat_model, model_sat_seg=sat_seg_model, loss=InfoNCE(loss_function=nn.CrossEntropyLoss(), logit_scale=5.0), multiple_losses=False)"
      ],
      "metadata": {
        "id": "YApHu9J0_Sva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Trainer\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"quadruple_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='quadruple_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs=100,\n",
        "    devices=1,\n",
        "    callbacks=[RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alcJP_ZfA8kS",
        "outputId": "19cd0899-ba13-4408-fb96-ce24a39a63fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441,
          "referenced_widgets": [
            "a4fa4b355d5a4704902e27348b78fd24",
            "98c7ec191eb34202ae401bf820c5675e"
          ]
        },
        "id": "Xz5-lu_lA-Kv",
        "outputId": "c0253cb6-4a59-46b2-8095-47eb3e336ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: tb_logs/quadruple_model\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ loss    │ InfoNCE      │      0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch1 │ ResNetBranch │ 43.5 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ branch2 │ ResNetBranch │ 43.5 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ branch3 │ ResNetBranch │ 43.5 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ branch4 │ ResNetBranch │ 43.5 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ fc_grd  │ Linear       │  1.0 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ fc_sat  │ Linear       │  1.0 M │\n",
              "└───┴─────────┴──────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ loss    │ InfoNCE      │      0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch1 │ ResNetBranch │ 43.5 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ branch2 │ ResNetBranch │ 43.5 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ branch3 │ ResNetBranch │ 43.5 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ branch4 │ ResNetBranch │ 43.5 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ fc_grd  │ Linear       │  1.0 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ fc_sat  │ Linear       │  1.0 M │\n",
              "└───┴─────────┴──────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 176 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 176 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 705                                                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 176 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 176 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 705                                                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4fa4b355d5a4704902e27348b78fd24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test\n",
        "trainer.test(\n",
        "    dataloaders=val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285,
          "referenced_widgets": [
            "f9b7f1e82b6d495da5faf2905b517624",
            "120de11b263144339f076a601255b9c0"
          ]
        },
        "id": "2OInjw71A_xp",
        "outputId": "97a52d1e-d62d-4d7a-f02f-a9ef5d201f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:186: .test(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9b7f1e82b6d495da5faf2905b517624"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top1        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.004514672793447971   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top1%        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06952595710754395   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top10        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.027539502829313278   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top3        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.009480812586843967   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top1         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.004514672793447971    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top1%         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.06952595710754395    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top10         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.027539502829313278    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top3         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.009480812586843967    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_top1': 0.004514672793447971,\n",
              "  'test_top3': 0.009480812586843967,\n",
              "  'test_top10': 0.027539502829313278,\n",
              "  'test_top1%': 0.06952595710754395}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quintuple Model (RGB + SEG + DEPTH Grd | RGB + SEG Sat)"
      ],
      "metadata": {
        "id": "0F2TfTrGIKju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1TD552ZImNnnNAsVFQvzGe4oVYxkC_nBE' />\n",
        "<figcaption>Quintuple Model architecture using ground and satellite segmentation + ground depth estimation</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "goFmBE3sHGnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Quintuple Model\n",
        "\n",
        "class QuintupleModel(MultiBranchModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_grd_seg,\n",
        "        model_grd_depth,\n",
        "        model_sat,\n",
        "        model_sat_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        multiple_losses=True,\n",
        "        fully_concat_grd=True,\n",
        "        fully_concat_sat=True,\n",
        "        loss_decay_rate = 0.65\n",
        "    ):\n",
        "        super(QuintupleModel, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_grd_seg\n",
        "        self.branch3 = model_grd_depth\n",
        "        self.branch4 = model_sat\n",
        "        self.branch5 = model_sat_seg\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch1.output_dim + self.branch2.output_dim + self.branch3.output_dim != self.branch4.output_dim + self.branch5.output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch1.output_dim + self.branch2.output_dim + self.branch3.output_dim\n",
        "\n",
        "        self.fully_concat_grd = fully_concat_grd\n",
        "        self.fully_concat_sat = fully_concat_sat\n",
        "\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # To make concatenations learnable\n",
        "        if self.fully_concat_grd:\n",
        "          self.fc_grd = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        if self.fully_concat_sat:\n",
        "          self.fc_sat = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.loss_decay_rate = loss_decay_rate\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_img, grd_seg, grd_depth, sat_img, sat_seg = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_rgb_emb = self.branch1(grd_img['imgs'])\n",
        "        grd_seg_emb = self.branch2(grd_seg['imgs'])\n",
        "        grd_dep_emb = self.branch3(grd_depth['imgs'])\n",
        "        sat_rgb_emb = self.branch4(sat_img['imgs'])\n",
        "        sat_seg_emb = self.branch5(sat_seg['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        grd_emb = torch.cat((grd_rgb_emb, grd_seg_emb, grd_dep_emb), dim=1)\n",
        "        sat_emb = torch.cat((sat_rgb_emb, sat_seg_emb), dim=1)\n",
        "        if self.fully_concat_grd:\n",
        "          grd_emb = self.fc_grd(grd_emb)\n",
        "        if self.fully_concat_sat:\n",
        "          sat_emb = self.fc_sat(sat_emb)\n",
        "\n",
        "        return (grd_rgb_emb, grd_seg_emb, grd_dep_emb, sat_rgb_emb, sat_seg_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[0], embeddings[1])\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[0], embeddings[2])\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[1], embeddings[2])\n",
        "            loss += self.loss_decay_rate*self.loss(embeddings[3], embeddings[4])\n",
        "\n",
        "            self.loss_decay_rate*= self.loss_decay_rate\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"QuintupleModel(model_grd={self.branch1}, model_grd_seg={self.branch2},\n",
        "        model_grd_depth={self.branch3}, model_sat={self.branch4}, model_sat_seg={self.branch5},\n",
        "        loss={self.loss}, multiple_losses={self.multiple_losses},\n",
        "        fully_concat_grd={self.fully_concat_grd}, fully_concat_sat={self.fully_concat_sat}, loss_decay_rate = {self.loss_decay_rate})\"\"\""
      ],
      "metadata": {
        "id": "zOK0sSntasYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Model\n",
        "\n",
        "grd_model = ResNetBranch(output_dim = 256, resnet_version = 50) # LOOKS PROMISING!!!\n",
        "grd_seg_model = ResNetBranch(output_dim = 256, resnet_version = 50)\n",
        "grd_depth_model = ResNetBranch(output_dim = 256, resnet_version = 50)\n",
        "sat_model = ResNetBranch(output_dim = 384,  resnet_version = 50)\n",
        "sat_seg_model = ResNetBranch(output_dim = 384,  resnet_version = 50)\n",
        "\n",
        "\"\"\"\n",
        "grd_model = SAIGBranch(\n",
        "    data_module.size['grd'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "grd_seg_model = SAIGBranch(\n",
        "    data_module.size['grd_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "grd_depth_model = SAIGBranch(\n",
        "    data_module.size['grd_depth'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "sat_model = SAIGBranch(\n",
        "    data_module.size['sat'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 9\n",
        ")\n",
        "\n",
        "sat_seg_model = SAIGBranch(\n",
        "    data_module.size['sat_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 9\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "model = QuintupleModel(model_grd=grd_model, model_grd_seg=grd_seg_model, model_grd_depth=grd_depth_model, model_sat=sat_model, model_sat_seg=sat_seg_model, loss=InfoNCE(loss_function=nn.CrossEntropyLoss(), logit_scale=5.0))"
      ],
      "metadata": {
        "id": "cSkz4jbRLsJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Trainer\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"quintuple_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='quintuple_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs=100,\n",
        "    devices=1,\n",
        "    callbacks=[RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCvovcR0MU3t",
        "outputId": "0a7dd40e-820a-45a1-ab8d-0e6413604cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "26acdefac7ad4ee1ab2798d4bd3b4758",
            "589efa726f1844a1b7d1eb4359ba0e43"
          ]
        },
        "id": "RFqhLQjxMWLa",
        "outputId": "5b55907d-a867-476f-fab6-fa44bfa46ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ loss    │ InfoNCE      │      0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch1 │ ResNetBranch │ 24.0 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ branch2 │ ResNetBranch │ 24.0 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ branch3 │ ResNetBranch │ 24.0 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ branch4 │ ResNetBranch │ 24.3 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ branch5 │ ResNetBranch │ 24.3 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ fc_grd  │ Linear       │  590 K │\n",
              "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ fc_sat  │ Linear       │  590 K │\n",
              "└───┴─────────┴──────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ loss    │ InfoNCE      │      0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch1 │ ResNetBranch │ 24.0 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ branch2 │ ResNetBranch │ 24.0 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ branch3 │ ResNetBranch │ 24.0 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ branch4 │ ResNetBranch │ 24.3 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ branch5 │ ResNetBranch │ 24.3 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ fc_grd  │ Linear       │  590 K │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>│ fc_sat  │ Linear       │  590 K │\n",
              "└───┴─────────┴──────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 121 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 121 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 487                                                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 121 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 121 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 487                                                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26acdefac7ad4ee1ab2798d4bd3b4758"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test\n",
        "trainer.test(\n",
        "    dataloaders=val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ],
      "metadata": {
        "id": "36YgM7dBMXli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Functions"
      ],
      "metadata": {
        "id": "gJAKm9G3-Kwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Heatmap\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def create_activation_maps(\n",
        "    model,\n",
        "    data_module,\n",
        "    split,\n",
        "    save_dir,\n",
        "    use_gpu = True\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    spacing = 10\n",
        "    fading = 0.5\n",
        "\n",
        "    if split == 'train':\n",
        "        data_loader = data_module.train_dataloader()\n",
        "    elif split == 'test':\n",
        "        data_loader = data_module.test_dataloader()\n",
        "    elif split == 'val':\n",
        "        data_loader = data_module.val_dataloader()\n",
        "    else:\n",
        "        raise ValueError('split should be \"train\", \"test\" or \"val\"')\n",
        "\n",
        "    grd_mean = data_module.mean['grd']\n",
        "    grd_std = data_module.std['grd']\n",
        "    sat_mean = data_module.mean['sat']\n",
        "    sat_std = data_module.std['sat']\n",
        "\n",
        "    grd_height, grd_width = data_module.size['grd']\n",
        "    sat_height, sat_width = data_module.size['sat']\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        grd_imgs, grd_ids = batch[0]['imgs'], batch[0]['imgs_id']\n",
        "        sat_imgs, sat_ids = batch[3]['imgs'], batch[3]['imgs_id']\n",
        "\n",
        "        if use_gpu:\n",
        "            grd_imgs = grd_imgs.cuda()\n",
        "            sat_imgs = sat_imgs.cuda()\n",
        "\n",
        "        grd_output = model.branch1(grd_imgs, featuremaps=True)\n",
        "        sat_output = model.branch2(sat_imgs, featuremaps=True)\n",
        "\n",
        "        # compute activation maps for streetview (try adding square root?)\n",
        "        grd_output = (grd_output**2).sum(1)\n",
        "        b, h, w = grd_output.size()\n",
        "        grd_output = grd_output.view(b, h * w)\n",
        "        grd_output = nn.functional.normalize(grd_output, p=2, dim=1)\n",
        "        grd_output = grd_output.view(b, h, w)\n",
        "\n",
        "        # compute activation maps for satmap\n",
        "        sat_output = (sat_output**2).sum(1)\n",
        "        b, h, w = sat_output.size()\n",
        "        sat_output = sat_output.view(b, h * w)\n",
        "        sat_output = nn.functional.normalize(sat_output, p=2, dim=1)\n",
        "        sat_output = sat_output.view(b, h, w)\n",
        "\n",
        "        if use_gpu:\n",
        "            grd_imgs, grd_output = grd_imgs.cpu(), grd_output.cpu()\n",
        "            sat_imgs, sat_output = sat_imgs.cpu(), sat_output.cpu()\n",
        "\n",
        "        for index in range(grd_output.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            img_id = str(int(grd_ids[index])).zfill(7)\n",
        "\n",
        "            # RGB image (from the normalized input image)\n",
        "            input_img = grd_imgs[index, ...]\n",
        "            for img, mean, std in zip(input_img, grd_mean, grd_std):\n",
        "                img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "            input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "            input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "            # activation map (from the output image)\n",
        "            act_map = grd_output[index, ...].numpy()\n",
        "            act_map = cv2.resize(act_map, (grd_width, grd_height))\n",
        "            act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "            act_map = np.uint8(np.floor(act_map))\n",
        "            act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapping between the two images\n",
        "            overlapped_img = input_img*(1-fading) + act_map*fading\n",
        "            overlapped_img[overlapped_img > 255] = 255\n",
        "            overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            output_img = 255 * np.ones((3*grd_height + 2*spacing, grd_width, 3), dtype=np.uint8)\n",
        "            output_img[:grd_height, ...] = input_img[..., ::-1]\n",
        "            output_img[grd_height + spacing:2*grd_height + spacing, ...] = act_map\n",
        "            output_img[2*grd_height + 2*spacing:, ...] = overlapped_img\n",
        "            cv2.imwrite(os.path.join(save_dir, img_id + '_streetview.jpg'), output_img)\n",
        "\n",
        "        for index in range(sat_output.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            img_id = str(int(sat_ids[index])).zfill(7)\n",
        "\n",
        "            # RGB image (input image)\n",
        "            input_img = sat_imgs[index, ...]\n",
        "            for img, mean, std in zip(input_img, sat_mean, sat_std):\n",
        "                img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "            input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "            input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "            # activation map\n",
        "            act_map = sat_output[index, ...].numpy()\n",
        "            act_map = cv2.resize(act_map, (sat_width, sat_height))\n",
        "            act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "            act_map = np.uint8(np.floor(act_map))\n",
        "            act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapped image\n",
        "            overlapped_img = input_img*(1-fading) + act_map*(fading)\n",
        "            overlapped_img[overlapped_img > 255] = 255\n",
        "            overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            output_img = 255 * np.ones((3*sat_height + 2*spacing, sat_width, 3), dtype=np.uint8)\n",
        "            output_img[:sat_height, ...] = input_img[..., ::-1]\n",
        "            output_img[sat_height + spacing:2*sat_height + spacing, ...] = act_map\n",
        "            output_img[2*sat_height + 2*spacing:, ...] = overlapped_img\n",
        "            cv2.imwrite(os.path.join(save_dir, img_id + '_satmap.jpg'), output_img)\n",
        "        # Restore the original mode of the model\n",
        "        if was_training:\n",
        "          model.train()"
      ],
      "metadata": {
        "id": "36uMaOGy-NFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Ranked Results\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_ranked_results(\n",
        "    model,\n",
        "    data_module,\n",
        "    split,\n",
        "    save_dir,\n",
        "    top_k = 5,\n",
        "    use_gpu = True\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    spacing = 10\n",
        "    query_spacing = 30\n",
        "    border = 5\n",
        "    text_space = 30\n",
        "\n",
        "    # select dataloader\n",
        "    if split == 'train':\n",
        "        data_loader = data_module.train_dataloader()\n",
        "    elif split == 'test':\n",
        "        data_loader = data_module.test_dataloader()\n",
        "    elif split == 'val':\n",
        "        data_loader = data_module.val_dataloader()\n",
        "    else:\n",
        "        raise ValueError('split should be \"train\", \"test\" or \"val\"')\n",
        "\n",
        "    # (using data module dimensions)\n",
        "    grd_height, grd_width = data_module.original_size['grd']\n",
        "    sat_height, sat_width = data_module.original_size['sat']\n",
        "\n",
        "    grd_ids = np.empty((0))\n",
        "    sat_ids = np.empty((0))\n",
        "\n",
        "    model.grd_features_test = []\n",
        "    model.sat_features_test = []\n",
        "\n",
        "    # compute features for each image\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        grd_ids = np.concatenate((grd_ids, batch[0]['imgs_id']))\n",
        "        sat_ids = np.concatenate((sat_ids, batch[0]['imgs_id']))\n",
        "\n",
        "        if use_gpu:\n",
        "            batch[0]['imgs'] = batch[0]['imgs'].cuda()\n",
        "            batch[1]['imgs'] = batch[1]['imgs'].cuda()\n",
        "            batch[2]['imgs'] = batch[2]['imgs'].cuda()\n",
        "            batch[3]['imgs'] = batch[3]['imgs'].cuda()\n",
        "            batch[4]['imgs'] = batch[4]['imgs'].cuda()\n",
        "\n",
        "        model.test_step(batch, batch_idx)\n",
        "\n",
        "        if use_gpu:\n",
        "            batch[0]['imgs'] = batch[0]['imgs'].cpu()\n",
        "            batch[1]['imgs'] = batch[1]['imgs'].cpu()\n",
        "            batch[2]['imgs'] = batch[2]['imgs'].cpu()\n",
        "            batch[3]['imgs'] = batch[3]['imgs'].cuda()\n",
        "            batch[4]['imgs'] = batch[4]['imgs'].cuda()\n",
        "\n",
        "    grd_features = torch.cat(model.grd_features_test, dim = 0)\n",
        "    sat_features = torch.cat(model.sat_features_test, dim = 0)\n",
        "\n",
        "    if use_gpu:\n",
        "        grd_features = grd_features.cpu()\n",
        "        sat_features = sat_features.cpu()\n",
        "\n",
        "    num_samples = grd_features.shape[0]\n",
        "\n",
        "    model.grd_features = []\n",
        "    model.sat_features = []\n",
        "\n",
        "    # compute distance matrix\n",
        "    grd_features = F.normalize(grd_features, dim=-1)\n",
        "    sat_features = F.normalize(sat_features, dim=-1)\n",
        "    dist_matrix = 1 - grd_features @ sat_features.T\n",
        "    indices = np.argsort(dist_matrix, axis=1)\n",
        "\n",
        "    for grd_index in range(num_samples):\n",
        "\n",
        "        # create empty output image\n",
        "        output_height = grd_height + query_spacing + sat_height + text_space\n",
        "        output_width = max(grd_width, top_k*sat_width + (top_k-1)*spacing)\n",
        "        output_img = 255 * np.ones((output_height, output_width, 3), dtype=np.uint8)\n",
        "\n",
        "        # create query image with black border\n",
        "        grd_id = str(int(grd_ids[grd_index])).zfill(7)\n",
        "        grd_path = data_module.data_dir + '/streetview/' + grd_id + '.jpg'\n",
        "        color = (0,0,0)\n",
        "        grd_img = cv2.imread(grd_path)\n",
        "        grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "        grd_img = cv2.copyMakeBorder(grd_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "        grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "\n",
        "        # add query image to the output\n",
        "        start_width = (output_width - grd_width) // 2\n",
        "        end_width = start_width + grd_width\n",
        "        output_img[:grd_height, start_width:end_width, :] = grd_img\n",
        "\n",
        "        rank = 1\n",
        "        for sat_index in indices[grd_index, :]:\n",
        "\n",
        "            # create ranked image with red or green border\n",
        "            sat_id = str(int(sat_ids[sat_index])).zfill(7)\n",
        "            folder = 'polarmap/normal' if data_module.polar else 'bingmap'\n",
        "            sat_path = data_module.data_dir + '/' + folder + '/input' + sat_id + '.png'\n",
        "            color = (0, 255, 0) if (grd_id == sat_id) else (0, 0, 255)\n",
        "            sat_img = cv2.imread(sat_path)\n",
        "            sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "            sat_img = cv2.copyMakeBorder(sat_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "            sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "\n",
        "            # add ranked image to the output\n",
        "            start_height = grd_height+query_spacing\n",
        "            end_height = start_height + sat_height\n",
        "            start_width = (rank-1) * (sat_width + spacing)\n",
        "            end_width = start_width + sat_width\n",
        "            output_img[start_height:end_height, start_width:end_width, :] = sat_img\n",
        "\n",
        "            # add text about the ranked image\n",
        "            text = \"Rank: {} Distance:{:.4f}\".format(rank, dist_matrix[grd_index, sat_index])\n",
        "            bottom_left = (start_width + 10, output_height - 5)\n",
        "            cv2.putText(output_img, text, bottom_left, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 1, 2)\n",
        "\n",
        "            rank += 1\n",
        "            if rank > top_k:\n",
        "                break\n",
        "\n",
        "        # create output image file\n",
        "        cv2.imwrite(os.path.join(save_dir, grd_id + '_visrank.jpg'), output_img)\n",
        "\n",
        "        if was_training:\n",
        "          model.train()"
      ],
      "metadata": {
        "id": "xRMabKiQQffN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r '/content/output/log/activation_maps'\n",
        "!mkdir '/content/output/log/activation_maps'\n",
        "\n",
        "create_activation_maps(\n",
        "    model = model.to(device),\n",
        "    data_module = data_module,\n",
        "    split = 'val',\n",
        "    save_dir = '/content/output/log/activation_maps',\n",
        "    use_gpu = True\n",
        ")"
      ],
      "metadata": {
        "id": "5dSp6VMiKNAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r '/content/output/log/ranked_results'\n",
        "!mkdir '/content/output/log/ranked_results'\n",
        "\n",
        "visualize_ranked_results(\n",
        "    model = model.to(device),\n",
        "    data_module = data_module,\n",
        "    split = 'val',\n",
        "    save_dir = '/content/output/log/ranked_results',\n",
        "    top_k = 5,\n",
        "    use_gpu = True\n",
        ")"
      ],
      "metadata": {
        "id": "buh9sdrgGU-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ],
      "metadata": {
        "id": "BvaU8HmOln47"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pi5yzhh_RW7B",
        "y1FRSut8m-Mv",
        "pDSCF0zinL8Y",
        "5VmODtjKnRzH",
        "rXEBXiWchDAq",
        "wNh1Gez5-j8h",
        "RouQUwE2nWua",
        "-eQpKOJZarI6",
        "ZGDT8x2WPpbC",
        "h9YaK9QS2VN5",
        "0F2TfTrGIKju"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0debe9663844313aecbdfad4f3beb05": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8941ac94632446edacfb19b79293caec",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m139/139\u001b[0m \u001b[38;5;245m0:01:29 • 0:00:00\u001b[0m \u001b[38;5;249m1.58it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">139/139</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:29 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.58it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "8941ac94632446edacfb19b79293caec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287eaf59c6ca428ba5febacfab676fa0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_86b79eba4b78405c84a186fdca6ad3b3",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 1/99 \u001b[38;2;98;6;224m━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m77/416\u001b[0m \u001b[38;5;245m0:01:03 • 0:04:38\u001b[0m \u001b[38;5;249m1.22it/s\u001b[0m \u001b[37mv_num: 0.000 train_top1_step: 0.688\u001b[0m\n                                                                                \u001b[37mtrain_loss_step: 1.398 val_top1:   \u001b[0m\n                                                                                \u001b[37m0.016 val_top3: 0.053 val_top10:   \u001b[0m\n                                                                                \u001b[37m0.144 val_top1%: 0.276             \u001b[0m\n                                                                                \u001b[37mtrain_top1_epoch: 0.473            \u001b[0m\n                                                                                \u001b[37mtrain_loss_epoch: 2.569            \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 1/99 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">77/416</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:03 • 0:04:38</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.22it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_top1_step: 0.688</span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: 1.398 val_top1:   </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.016 val_top3: 0.053 val_top10:   </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.144 val_top1%: 0.276             </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_top1_epoch: 0.473            </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_epoch: 2.569            </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "86b79eba4b78405c84a186fdca6ad3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec9b350c9af40f6b10ed9a476873c82": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5172e04dbc31440aa3cf65040c50830f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m139/139\u001b[0m \u001b[38;5;245m0:01:30 • 0:00:00\u001b[0m \u001b[38;5;249m1.67it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">139/139</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:30 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.67it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "5172e04dbc31440aa3cf65040c50830f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4fa4b355d5a4704902e27348b78fd24": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_98c7ec191eb34202ae401bf820c5675e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mEpoch 0/99\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m174/416\u001b[0m \u001b[38;5;245m0:03:50 • 0:04:47\u001b[0m \u001b[38;5;249m0.84it/s\u001b[0m \u001b[37mv_num: 0.000 train_top1_step:     \u001b[0m\n                                                                                 \u001b[37m0.438 train_loss_step: 1.481      \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/99</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">174/416</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:03:50 • 0:04:47</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.84it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_top1_step:     </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.438 train_loss_step: 1.481      </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "98c7ec191eb34202ae401bf820c5675e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9b7f1e82b6d495da5faf2905b517624": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_120de11b263144339f076a601255b9c0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m139/139\u001b[0m \u001b[38;5;245m0:01:39 • 0:00:00\u001b[0m \u001b[38;5;249m1.44it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">139/139</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:39 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.44it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "120de11b263144339f076a601255b9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26acdefac7ad4ee1ab2798d4bd3b4758": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_589efa726f1844a1b7d1eb4359ba0e43",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mEpoch 0/99\u001b[0m \u001b[38;2;98;6;224m━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m17/416\u001b[0m \u001b[38;5;245m0:00:21 • 0:06:51\u001b[0m \u001b[38;5;249m0.97it/s\u001b[0m \u001b[37mv_num: 1.000 train_top1_step: 0.312\u001b[0m\n                                                                                \u001b[37mtrain_loss_step: 8.412             \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/99</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">17/416</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:21 • 0:06:51</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.97it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 1.000 train_top1_step: 0.312</span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: 8.412             </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "589efa726f1844a1b7d1eb4359ba0e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d42b4eebd2458b90d8482751880a19": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_075e43ab72194bbdae38d001dc2a6f2e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mEpoch 0/29\u001b[0m \u001b[38;2;98;6;224m━━━\u001b[0m\u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m46/416\u001b[0m \u001b[38;5;245m0:00:29 • 0:03:33\u001b[0m \u001b[38;5;249m1.74it/s\u001b[0m \u001b[37mv_num: 0.000 train_top1_step: 0.250\u001b[0m\n                                                                                \u001b[37mtrain_loss_step: 2.092             \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/29</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">46/416</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:29 • 0:03:33</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.74it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_top1_step: 0.250</span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: 2.092             </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "075e43ab72194bbdae38d001dc2a6f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d6f985ccf14efa953b9e67322637dc": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c7e1e1b557464f1db99d930b9d04d0e0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m139/139\u001b[0m \u001b[38;5;245m0:01:03 • 0:00:00\u001b[0m \u001b[38;5;249m2.16it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">139/139</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:03 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.16it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c7e1e1b557464f1db99d930b9d04d0e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}